{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRI Machine Learning Workshop\n",
    "\n",
    "<b> 0) Overview of machine learning</b>\n",
    "\n",
    "<b>1) Introduction to scikit-learn</b>    \n",
    "    \n",
    "<b>2) Introduction to nilearn</b>  \n",
    "\n",
    "<b>3) Introduction to neuropredict</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements/Sources:\n",
    "\n",
    "Some of these examples are based on examples from the following very helpful resources:\n",
    "* The [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas\n",
    "* The [scikit-learn documentation](https://scikit-learn.org/stable/)  \n",
    "\n",
    "Another great resource:\n",
    "* [Andrew Ng's Machine Learning course on Coursera](https://www.coursera.org/learn/machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning\n",
    "\n",
    "## What is machine learning?\n",
    "\n",
    "Machine learning is when a computer learns from <b>experience E</b> with respect to some <b>task T</b> and some <b>performance measure P</b>. If performance (as measured by P) on task T improves with experience E, the computer program is said to learn. - Tom Mitchell (computer scientist at Carnegie Mellon University)\n",
    "\n",
    "<b>Example: recognizing spam emails</b>  \n",
    "Task: classfying emails as spam or not spam  \n",
    "Experience: from emails that are already labeled as spam or not spam  \n",
    "Performance: fraction of emails correctly classified\n",
    "\n",
    "Machine learning allows for <b>pattern recognition</b> and <b>prediction</b>.\n",
    "\n",
    "## Different types of machine learning\n",
    "\n",
    "* <b>Supervised learning</b>: learn labels for different classes based on sets of features\n",
    "    * \"teach\" the computer how to do something\n",
    "* <b>Unsupervised learning</b>: learn hidden structures in a set of features\n",
    "    * allow the computer to \"learn\" on its own\n",
    "* <b>Reinforcement learning</b>: given a sequence of states and actions with rewards, determine a policy, i.e. a mapping from states to actions (what action should you perform in a given state?)\n",
    "    * e.g. chess: \n",
    "        * state = position of pieces on the board\n",
    "        * action = what is the best move to make given the state?\n",
    "        * reward = winning the game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning\n",
    "\n",
    "\n",
    "## How does supervised machine learning work?\n",
    "\n",
    "1) First, we need to represent our data as a set of <b>observations</b> and <b>features</b>\n",
    "* Make a matrix where the rows are observations and the columns are different features\n",
    "\n",
    "2) Then, we need to define our <b>outputs</b>\n",
    "* If the output is continuous, we will run a <b>regression</b> algorithm\n",
    "* If the output is categorical (labels), we will run a <b>classification</b> algorithm\n",
    "    \n",
    "How do we represent categorical outputs?  \n",
    "With 1s and 0s.\n",
    "\n",
    "What if we have more than two classes?  \n",
    "With \"one-hot encoding\".\n",
    "\n",
    "3) We will need to partition our data into two or three subsets.\n",
    "    a) The first set is called the <b>training set</b>. This will be used to train our model.\n",
    "    b) The second set is called the <b>validation set</b>. We will need to use a validation set if we are training any <b>hyperparameters</b> in our model. Hyperparameters are parameters in the model that that model does not learn; they must be set beforehand. However, different values of these parameters can affect the model performance, so we want to see which value is associated with the best performance on the validation set.\n",
    "    c) The final set is called the <b>test set</b>. This set is used as a final dataset to see how well the trained model performs on data that it has never seen before. \n",
    "\n",
    "4) Next, we define a function that will allow us to map our features to our outputs/labels\n",
    "\n",
    "5) We also need to define a function that allows us to evaluate how well the function in 4) is performing. In other words, what is the difference, or <b>error</b>, between the outputs that are predicted by our model, and the actual outputs? The function we define here is called a <b>cost function</b> or <b>loss function</b>.\n",
    "\n",
    "6) We then want to <b>minimize</b> the error: we want the difference between the predicted and actual outputs to be as small as possible. One way of doing this is with an optimization algorithm called <b>gradient descent</b>. \n",
    "* We can find the minimum of the function by moving in the negative direction of the gradient (the slope of the loss function at a given point) to reach the local/global minima.\n",
    "\n",
    "7) Once we use gradient descent to find the <b>optimal model parameters</b>, we can <b>evaluate</b> the performance of our final model on a <b>left-out</b> subset of the data (i.e. the <b>test set</b>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run through an example of linear regression\n",
    "\n",
    "We want to predict a set of continuous outputs from a set of continuous inputs.\n",
    "We'll start simple: let's just work with one feature.\n",
    "\n",
    "First, import some packages that we'll need to run linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np, pandas as pd, seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "* <b>Goal</b>: predict continuous scores Y from continuous scores X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some data\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(1000)\n",
    "y = 1.34*x + 0.52\n",
    "print('original shape of x:', x.shape)\n",
    "print('original shape of y:', y.shape)\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "print('new shape of x:', x.shape)\n",
    "print('new shape of y:', y.shape)\n",
    "\n",
    "noise = np.random.randn(x.shape[0])\n",
    "noise = (noise - np.mean(noise))/np.std(noise)\n",
    "noise = noise[:, np.newaxis]\n",
    "y = y + noise\n",
    "\n",
    "plt.scatter(x,y,s=20,alpha=.5,c='c'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to define a function to map our x-values to our y-values. In the case of linear regression, we simply use the equation for a line: i.e. y = mx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our function for prediction\n",
    "\n",
    "def predict(x,m,b):\n",
    "    ypred = m*x + b\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our cost/loss function. For linear regression, we will use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "\n",
    "def calc_loss(y, ypred):\n",
    "    loss = 1/y.shape[0] * np.sum((y - ypred)**2)\n",
    "    #loss = 1/y.shape[0] * np.sum((y - (m*x + b))**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to take the derivative of the loss function with respect to each parameter that we want to optimize (m and b). In other words, we take the <b>partial derivatives</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivative w.r.t. m\n",
    "def calc_grad_m(x, y, ypred):\n",
    "    grad_m = (-2/y.shape[0]) * np.sum(x*(y - ypred))\n",
    "    return grad_m\n",
    "    \n",
    "# partial derivative w.r.t. b\n",
    "def calc_grad_b(y, pred):\n",
    "    grad_b = (-2/y.shape[0]) * np.sum((y - ypred))\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a <b>contour plot</b> of what the loss will be for different combinations of m and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make contour plot of losses\n",
    "\n",
    "def make_contours(WW,BB,X,Y):\n",
    "    nsteps = np.shape(WW)[0]\n",
    "    z = np.zeros((nsteps, nsteps))\n",
    "    for i in range(nsteps):\n",
    "        for j in range(nsteps):\n",
    "            ypred = predict(X, WW[i,j], BB[i,j])\n",
    "            z[i,j] = calc_loss(Y, ypred)\n",
    "    return z\n",
    "\n",
    "nsteps = 100\n",
    "cl = 10\n",
    "ww = np.linspace(-cl, cl, nsteps)\n",
    "cl = 10\n",
    "bb = np.linspace(-cl, cl, nsteps)\n",
    "WW, BB = np.meshgrid(ww,bb)\n",
    "z = make_contours(WW, BB, x, y)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "plt.xlim(-cl,cl); plt.ylim(-cl,cl)\n",
    "plt.contour(WW, BB, z, cmap='viridis')\n",
    "plt.xlabel('w'); plt.ylabel('b')\n",
    "plt.title('Linear Regression Loss')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run gradient descent.  \n",
    "We will start by randomly initializing `m` and `b`.   \n",
    "Then, we will define the number of iterations (`iters`) and the learning rate/step size for gradient descent (`learning_rate`).  \n",
    "\n",
    "For each iteration of gradient descent:  \n",
    "1) Predict y-values based on the current values of m and b and our data (x).  \n",
    "2) Calculate the overall loss, and the partial derivatives for m and b.  \n",
    "3) Update m and b by subtracting the respective loss, weighted by the learning rate.  \n",
    "\n",
    "After the final iteration: make final predictions of the y values based on the optimized values of m and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our parameters \n",
    "m = np.random.rand(1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# define iterations and learning rate\n",
    "iters = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# run gradient descent\n",
    "losses = []\n",
    "grads_m = []\n",
    "grads_b = []\n",
    "all_m = []\n",
    "all_b = []\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # make predictions\n",
    "    ypred = predict(x, m, b)\n",
    "    \n",
    "    # calculate loss and partial derivatives\n",
    "    loss = calc_loss(y, ypred)\n",
    "    grad_m = calc_grad_m(x, y, ypred)\n",
    "    grad_b = calc_grad_b(y, ypred)\n",
    "    \n",
    "    # update m and b\n",
    "    m = m - learning_rate*grad_m\n",
    "    b = b - learning_rate*grad_b\n",
    "    \n",
    "    # append values from this iteration\n",
    "    losses.append(loss)\n",
    "    grads_m.append(grad_m)\n",
    "    grads_b.append(grad_b)\n",
    "    all_m.append(m)\n",
    "    all_b.append(b)\n",
    "    \n",
    "# final ypred\n",
    "ypred = predict(x, m, b)\n",
    "\n",
    "print('Final loss (m): ', grads_m[-1])\n",
    "print('Final loss (b): ', grads_b[-1])\n",
    "print('Final value of m: ', m)\n",
    "print('Final value of b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code in the above cell, we also kept track of the overall loss and the gradients. Let's plot these to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.subplot(131); plt.title('Loss');\n",
    "plt.plot(np.arange(iters), losses)\n",
    "plt.xlabel('# iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(132); plt.title('Derivative of loss w.r.t. m')\n",
    "plt.plot(np.arange(iters), grads_m)\n",
    "plt.xlabel('# iterations')\n",
    "plt.ylabel('Loss w.r.t. m')\n",
    "\n",
    "plt.subplot(133); plt.title('Derivative of loss w.r.t. b')\n",
    "plt.plot(np.arange(iters), grads_b)\n",
    "plt.xlabel('# iterations')\n",
    "plt.ylabel('Loss w.r.t. b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plots, the loss gets smaller with each iteration, as do the absolute values of the gradients.   \n",
    "\n",
    "Let's see what happens when we change the number of iterations and the learning rate. Try re-running the model with more and fewer iterations, and with a larger and smaller learning rate. What do you think will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.plot(x, ypred, c = 'r',label='Predicted')\n",
    "plt.scatter(x, y, s=20, alpha=.5, c='c', label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = 100\n",
    "m1, m2 = 0.75, 1.6\n",
    "b1, b2 = 0.1, .6\n",
    "ww = np.linspace(m1, m2, nsteps)\n",
    "bb = np.linspace(b1, b2, nsteps)\n",
    "WW, BB = np.meshgrid(ww,bb)\n",
    "z = make_contours(WW, BB, x, y)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.xlim(m1,m2); plt.ylim(b1,b2)\n",
    "plt.contour(WW, BB, z, cmap='viridis')\n",
    "plt.grid('off')\n",
    "plt.colorbar()\n",
    "#plt.plot(all_m, all_b, '--',color='grey')\n",
    "plt.scatter(all_m, all_b, color='black',s=2)\n",
    "plt.scatter(all_m[0],all_b[0],marker='x',c='green')\n",
    "plt.scatter(all_m[-1],all_b[-1],marker='x',c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with scikit-learn\n",
    "\n",
    "Scikit-learn is one of the primary libraries used for machine learning in Python.\n",
    "\n",
    "### Typical workflow:\n",
    "\n",
    "Prepare data  \n",
    "split into train/(validation)/test sets  \n",
    "create an instance of a classifier  \n",
    "fit the model  \n",
    "make predictions  \n",
    "evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a linear regression using scikit-learn\n",
    "\n",
    "# import the Linear Regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# make an instance of the class\n",
    "lm = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lm.fit(x,y)\n",
    "\n",
    "# predict y\n",
    "ypred_sklearn = lm.predict(x)\n",
    "\n",
    "# let's see what the parameters are \n",
    "print('scikit-learn m: ', lm.coef_)\n",
    "print('scikit-learn b: ', lm.intercept_)\n",
    "\n",
    "# compare to our code:\n",
    "print('Final value of m: ', m)\n",
    "print('Final value of b: ', b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data\n",
    "\n",
    "from sklearn.datasets import load_digits, load_iris\n",
    "digits = load_digits()\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris dataset: what are the features?\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try predicting petal width from petal length\n",
    "\n",
    "X = iris.data[:,2] # petal length\n",
    "y = iris.data[:,3] # petal width\n",
    "X = X[:,np.newaxis]\n",
    "y = y[:,np.newaxis]\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(X, y,'o',color='c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# first, split into train and test sets\n",
    "np.random.seed(5)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "# make instance of class\n",
    "lm = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lm.fit(Xtrain, ytrain)\n",
    "\n",
    "# make predictions\n",
    "ypred = lm.predict(Xtest)\n",
    "\n",
    "# print optimized paramters\n",
    "print(lm.coef_, lm.intercept_)\n",
    "\n",
    "# print score: for linear regression, the default is R-squared\n",
    "print(lm.score(Xtrain, ytrain))\n",
    "print(lm.score(Xtest, ytest))\n",
    "\n",
    "# plot test data and regression line\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(Xtest, ytest, 'o', color='c', label='Actual')\n",
    "plt.plot(Xtest, ypred, color='r', label='Predicted')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Predict categorical values Y from a set of continuous features X.\n",
    "\n",
    "\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "Use the <b>sigmoid activation function</b> to convert outputs to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "inds = np.logical_or(iris.target==0, iris.target==1)\n",
    "\n",
    "X = X[inds,0:2]\n",
    "y = y[inds]\n",
    "\n",
    "# standardize data so each column has min of 0, max of 1\n",
    "X = (X - np.min(X,axis=0)) / (np.max(X,axis=0) - np.min(X,axis=0))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],c=y,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our function for prediction\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "xx = np.linspace(-10,10,100)\n",
    "yy = sigmoid(xx)\n",
    "\n",
    "plt.plot(xx,yy); \n",
    "plt.plot([0,0],[0,1],c='black')\n",
    "plt.plot([-10, 10],[0.5, 0.5],c='black')\n",
    "plt.xlabel('Value before sigmoid')\n",
    "plt.ylabel('Value after sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "def predict(x,theta,bias):\n",
    "    ypred = sigmoid(np.matmul(x,theta)+bias)\n",
    "    return ypred[:,0]\n",
    "\n",
    "# loss\n",
    "def calc_loss(y, ypred):\n",
    "    loss = 1/y.shape[0] * np.sum(-y * np.log(ypred) - (1 - y) * np.log(1 - ypred))\n",
    "    return loss\n",
    "\n",
    "# gradient\n",
    "def calc_grad(x, y, ypred):\n",
    "    grad = 1/y.shape[0] * np.matmul(x.T, (ypred - y))\n",
    "    grad = grad[:, np.newaxis]\n",
    "    return grad\n",
    "\n",
    "# accuracy\n",
    "def calc_accuracy(y, ypred):\n",
    "    h = np.zeros(ypred.shape)\n",
    "    h[ypred>=0.5] = 1\n",
    "    return np.sum(h==y)/y.shape[0]\n",
    "\n",
    "\n",
    "# initialize our parameters \n",
    "theta = np.random.rand(2,1)\n",
    "bias = np.random.rand(1,1)\n",
    "\n",
    "# split into train and test\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y)\n",
    "\n",
    "# define iterations \n",
    "iters = 20000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "losses = []\n",
    "grads = []\n",
    "for iter in range(iters):\n",
    "    ypred = predict(Xtrain, theta, bias)\n",
    "    loss = calc_loss(ytrain, ypred)\n",
    "    grad = calc_grad(Xtrain, ytrain, ypred)\n",
    "    theta = theta - learning_rate*grad\n",
    "    bias = bias - learning_rate*(np.sum(ypred-ytrain))\n",
    "    losses.append(loss)\n",
    "    grads.append(grad)\n",
    "\n",
    "# final ypred\n",
    "ypred = predict(Xtest, theta, bias)\n",
    "grads = np.squeeze(grads)\n",
    "print(theta)\n",
    "print(bias)\n",
    "print(calc_accuracy(ytest, ypred))\n",
    "      \n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.subplot(131); plt.title('Loss')\n",
    "plt.plot(np.arange(iters), losses)\n",
    "plt.subplot(132); plt.title('Gradient')\n",
    "plt.plot(np.arange(iters), grads)\n",
    "plt.legend(['theta0', 'theta1'])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "#plt.plot(Xtest, ypred, c = 'r')\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1],c=ytest,cmap='coolwarm')\n",
    "xvals = np.array(plt.gca().get_xlim())\n",
    "xvals = np.reshape(xvals,[-1, 1])\n",
    "yvals = -(xvals*theta[0] + bias) /theta[1]\n",
    "plt.plot(xvals,yvals,c='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression in scikit-learn\n",
    "\n",
    "A few notes:\n",
    "\n",
    "* Regularization: constrains the weights to prevent them from getting too large; helps prevent overfitting the training data\n",
    "    * C: \"amount\" of regularization: Inverse of regularization strength, where smaller values = stronger regularization  \n",
    "    \n",
    "    \n",
    "    \n",
    "* Uses stochastic average gradient descent solver: https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/linear_model/logistic.py#L1176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "clf = LogisticRegression(penalty='l2', C=80.0, max_iter=20000, tol=1e-20)\n",
    "\n",
    "# fit to training data\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "# predict y\n",
    "ypred = clf.predict(Xtest)\n",
    "\n",
    "# evaluate model accuracy\n",
    "print('Accuracy = ', metrics.accuracy_score(ypred, ytest)*100, '%')\n",
    "\n",
    "print('Weights: ', clf.coef_)\n",
    "print('Intercept: ', clf.intercept_)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1],c=ytest,cmap='coolwarm')\n",
    "xvals = np.array(plt.gca().get_xlim())\n",
    "xvals = np.reshape(xvals,[-1, 1])\n",
    "yvals = -(xvals*clf.coef_[0][0] + clf.intercept_) /clf.coef_[0][1]\n",
    "plt.plot(xvals,yvals)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "<b>Source</b>: https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "Find boundaries that best separates data.  \n",
    "We want to maximize the margins that can be drawn around this boundary.\n",
    "In the case of a linear boundary, the margin the width between the boundary line and a parallel line that just touches the nearest data point.  \n",
    "\n",
    "Here is an illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "Xl, yl = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.6)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(Xl[:, 0], Xl[:, 1], c=yl, s=50, cmap='coolwarm', alpha=.8);\n",
    "x = np.linspace(-1, 4)\n",
    "y1 = 0.5*x+1.7\n",
    "y2 = 0.1*x + 2.5\n",
    "y3 = -0.2*x + 2.8\n",
    "margin1 = .2\n",
    "margin2 = .45\n",
    "margin3 = .25\n",
    "plt.plot(x,y1,c='purple')\n",
    "plt.plot(x,y2,c='orange')\n",
    "plt.plot(x,y3,c='green')\n",
    "plt.fill_between(x,y1-margin1,y1+margin1,color='purple',alpha=.2)\n",
    "plt.fill_between(x,y2-margin2,y2+margin2,color='orange',alpha=.2)\n",
    "plt.fill_between(x,y3-margin3,y3+margin3,color='green',alpha=.2)\n",
    "plt.xlim([-1, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we can see three examples of potential boundaries that are all successful in separating the two classes of datapoints (there are more than three possible lines!).  \n",
    "\n",
    "However, for each line, we can draw two surrounding lines (with equal margins on each side) to show how far away each boundary line is from the closest example in each class. (The margin has to be the same on both sides, so the width is equal to the distance between the boundary line and the closest data point.) \n",
    "\n",
    "We can clearly see that the orange line has the largest margin, so this would be the line that the SVM algorithm chooses!  \n",
    "\n",
    "SVMs are called <b>maximum margin estimators</b> for this reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs use <b>kernel functions</b> to define the boundaries. Kernels allow us to project our data into a higher-dimensional space. \n",
    "\n",
    "In the example above, the data can be separated using a <b>linear</b> kernel.  \n",
    "\n",
    "Let's take another dataset. This data is clearly not separable using a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc, yc = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(Xl[:, 0], Xl[:, 1], c=yl, s=50, cmap='coolwarm', alpha=.5);\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xc[:, 0], Xc[:, 1], c=yc, s=50, cmap='coolwarm', alpha=.5); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a <b>radial basis function</b> to project the data into a higher-dimensional space where it *can* be linearly separated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the radial basis function:\n",
    "\n",
    "r = np.exp(np.sum(-((Xc)**2),1))\n",
    "\n",
    "# plot the data projection\n",
    "from mpl_toolkits import mplot3d\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(Xc[:,0],Xc[:,1],r, c=yc, s=50, cmap='coolwarm')\n",
    "ax.set_xlabel('X1'), ax.set_ylabel('X2'), ax.set_zlabel('r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that we can use a linear kernel for the first dataset, and an RBF kernel for the second dataset.\n",
    "\n",
    "Let's see how we can run SVMs with different kernels in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the models\n",
    "model_linear = SVC(kernel='linear', C=1e10) # use a linear kernel\n",
    "model_rbf = SVC(kernel='rbf', C=1e10)\n",
    "\n",
    "# fit the models\n",
    "model_linear.fit(Xl, yl)\n",
    "model_rbf.fit(Xc, yc)\n",
    "\n",
    "# support vectors (define the margin)\n",
    "print(model_linear.support_vectors_,'\\n')\n",
    "print(model_rbf.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model):\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(Xl[:, 0], Xl[:, 1], c=yl, s=50, cmap='coolwarm', alpha=.5)\n",
    "plot_svc_decision_function(model_linear);\n",
    "plt.title('Linear Kernel')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(Xc[:, 0], Xc[:, 1], c=yc, s=50, cmap='coolwarm', alpha=.5)\n",
    "plot_svc_decision_function(model_rbf); \n",
    "plt.title('RBF Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits dataset\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(ypred, ytest)*100,'%')\n",
    "\n",
    "sns.heatmap(metrics.confusion_matrix(ypred, ytest), annot=True,\n",
    "           cmap=plt.cm.viridis)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other classification algorithms in sklearn that you can explore. Here are a few examples:\n",
    "\n",
    "* Decision Trees: `sklearn.tree.DecisionTreeClassifier`\n",
    "* Random Forests: `sklearn.ensemble.RandomForestClassifier`\n",
    "* Gaussian Naive Bayes (GNB): `sklearn.naive_bayes.GaussianNB`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our models: Cross-validation\n",
    "\n",
    "Recall from the logistic regression example that we randomly split our data into training and test sets. When the data are randomly partitioned, we can end up with different accuracies on the test set depending on which examples end up in the training and test sets.  \n",
    "\n",
    "Therefore, it is advisable to train and test our model on different training and test sets. This way, we can get an average measure of our model's performance. Especially when we have smaller datasets, if we only run our model with a single training set and a single test set, it is possible to over- or under-estimate how well our model can actually perform.  \n",
    "\n",
    "For instance, if by chance we end up with a test set that contains several outliers but a training set that does not contain any outliers, the performance on our test set will be quite low.  \n",
    "\n",
    "One method of cross-validation is called <b>k-fold cross validation</b>. With this method, we divide our data into *k* sets. We train our model on all but one of the *k* sets, and test the model on the remaining set. We repeat this process *k* times, each time leaving out the *k*th subset of data. Then, we can look at the accuracy for each fold, and take a summary measure, such as the mean or median of the *k* accuracy scores.  \n",
    "\n",
    "In the example below, we will use k=10, so we will end up with 10 accuracy scores.\n",
    "\n",
    "Note that all we have to do is define a classifier (`clf` below). The `cross_val_score` function will call on the `.fit` and `.predict` functions and keep track of the scores, so you don't have to do this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Run cross-validation: inputs = classifier, X, y, k\n",
    "cv_scores = cross_val_score(clf, digits.data, digits.target, cv=10)\n",
    "print('All CV scores: ', cv_scores)\n",
    "print('Mean CV accuracy = ', np.mean(cv_scores)*100, '%')\n",
    "print('SD of CV accuracy = ', np.std(cv_scores)*100, '%')\n",
    "print('Median CV accuracy = ', np.median(cv_scores)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make <b>validation curves</b> using sklearn.  \n",
    " \n",
    "\n",
    "Validation curves allow us to see average scores on training and validation sets for different values of our <b>hyperparameters</b>. Recall that hyperparameters are parameters in the model that we set ourselves, i.e. they are not learned/optimized during model training.  \n",
    "\n",
    "<b>Source</b>: https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation curve\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "gamma_values = np.logspace(-6, -1, 6)\n",
    "train_score, val_score = validation_curve(SVC(), digits.data, digits.target, 'gamma', gamma_values, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score.shape) # number of values * k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "\n",
    "plt.semilogx(gamma_values, np.mean(train_score, 1), 'o-', label=\"training score\",\n",
    "             color=\"purple\")\n",
    "plt.fill_between(gamma_values, np.mean(train_score, 1) - np.std(train_score, 1),\n",
    "                 np.mean(train_score, 1) + np.std(train_score, 1), alpha=0.2,\n",
    "                 color=\"purple\")\n",
    "plt.semilogx(gamma_values, np.mean(val_score, 1), 'o-', label=\"validation score\",\n",
    "             color=\"c\")\n",
    "plt.fill_between(gamma_values, np.mean(val_score, 1) - np.std(val_score, 1),\n",
    "                  np.mean(val_score, 1) + np.std(val_score, 1), alpha=0.2,\n",
    "                 color=\"c\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar concept is <b>learning curves</b>, which show training and validation scores as a function of training set size.  \n",
    "\n",
    "<b>Source</b>: https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "N, train_score, val_score = learning_curve(GaussianNB(), digits.data, digits.target, cv=5, \n",
    "                                                train_sizes=np.linspace(0.1, 0.9, 9))\n",
    "\n",
    "fig = plt.subplots(1, 1, figsize=(8, 6))\n",
    "plt.plot(N, np.mean(train_score, 1), 'o-', color='purple', label='training score')\n",
    "plt.fill_between(N, np.mean(train_score, 1) - np.std(train_score, 1),\n",
    "                 np.mean(train_score, 1) + np.std(train_score, 1), alpha=0.2,\n",
    "                 color=\"purple\")\n",
    "plt.plot(N, np.mean(val_score, 1), 'o-', color='c', label='validation score')\n",
    "plt.fill_between(N, np.mean(val_score, 1) - np.std(val_score, 1),\n",
    "                  np.mean(val_score, 1) + np.std(val_score, 1), alpha=0.2,\n",
    "                 color=\"c\")\n",
    "plt.ylim(0.5, 1)\n",
    "plt.xlim(N[0], N[-1])\n",
    "plt.xlabel('training size')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also has a tool called `GridSearchCV`, which allows us to search over a grid of parameters for the combination of parameters that yields the best validation score.\n",
    "\n",
    "We will also make a <b>pipeline</b>, which is helpful when we need to perform the same series of functions multiple times. There are two ways to do this in sklearn: `make_pipeline` or `Pipeline` (we will walk through both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV with pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = iris.data; y = iris.target\n",
    "\n",
    "# option 1: Pipeline (define names explicitly)\n",
    "pipe1 = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('reduce_dims', PCA()),\n",
    "        ('clf', SVC())])\n",
    "\n",
    "param_grid1 = dict(reduce_dims__n_components=[1,2,3],\n",
    "                  clf__C=np.logspace(-4, 1, 6),\n",
    "                  clf__kernel=['rbf','linear'])\n",
    "\n",
    "# option 2: make_pipeline (auto-generates names)\n",
    "pipe2 = make_pipeline(StandardScaler(),\n",
    "                         PCA(),\n",
    "                        SVC())\n",
    "\n",
    "param_grid2 = dict(pca__n_components=[1,2,3],\n",
    "                  svc__C=np.logspace(-4, 1, 6),\n",
    "                  svc__kernel=['rbf','linear'])\n",
    "\n",
    "grid = GridSearchCV(pipe1, param_grid=param_grid1, scoring=['accuracy'], refit ='accuracy', cv=3)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print('Best Parameters: ', grid.best_params_)\n",
    "print('Best Score: ', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "<b>Goal</b>: look for hidden patterns in the data (we don't have labels for the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that is used to find a smaller set of uncorrelated variables (principal components) from a higher-dimensional set of data. These principal components are sorted based on the proportion of variance in the original data that the components explain (recall Derek's workshop from last week!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for digits data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(10, 4))\n",
    "    for i in range(10):\n",
    "        axes[i].imshow(data[i].reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# instantiate PCA\n",
    "# can specificy number of components or % variance explained\n",
    "pca = PCA(n_components=0.90)\n",
    "pca.fit(X) \n",
    "print('N components: ' + str(pca.n_components_))\n",
    "\n",
    "reduced = pca.transform(X)\n",
    "filtered = pca.inverse_transform(reduced)\n",
    "\n",
    "plot_digits(X)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first two PCs\n",
    "\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "im = ax.scatter(reduced[:, 0], reduced[:, 1], c=y, cmap='tab10',s=40, alpha=.6);\n",
    "ax.set_title(\"First 2 PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.set_xticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.set_yticklabels([])\n",
    "fig.colorbar(im)\n",
    "\n",
    "# plot the cumulative percent variance explained \n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(np.arange(np.shape(pca.explained_variance_ratio_)[0])+1,np.cumsum(pca.explained_variance_ratio_))\n",
    "ax.scatter(np.arange(np.shape(pca.explained_variance_ratio_)[0])+1,np.cumsum(pca.explained_variance_ratio_),c='r')\n",
    "ax.set_xlabel('# components')\n",
    "ax.set_ylabel('% variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original shape: ', X.shape)\n",
    "print('reduced shape: ', reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression for original and reduced data\n",
    "for data in [X, reduced]:\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(data, y, random_state=50)\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(Xtrain,ytrain)\n",
    "    print(clf.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "Find *k* centroids in the data where each point is assigned to the cluster centroid.\n",
    "\n",
    "1) Initialize centroids randomly.  \n",
    "2) Assignment step: assign each point to the nearest centroid.  \n",
    "3) Update step: the new centroids are defined as the mean of the points that are currently assigned to that centroid.   \n",
    "4) Repeat assignment and update steps until the assignment of points doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "X, y_true = iris.data[:,2:], iris.target\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# fix order of clusters\n",
    "from scipy.stats import mode\n",
    "labels = np.zeros(y_kmeans.shape)\n",
    "for i in range(3):\n",
    "    mask = (y_kmeans==i)\n",
    "    labels[mask] = mode(y_true[mask])[0]\n",
    "\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(X[:,0], X[:,1], c=y_true, s=50, cmap='cool',alpha=.6)\n",
    "plt.title('True classes')\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(X[:,0], X[:,1], c=labels, s=50, cmap='cool',alpha=.6)\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:,0], centers[:,1],c='black',s=100,alpha=.8)\n",
    "plt.title('K-means clusters')\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X[:,0], X[:,1], c=(y_true==labels), s=50, cmap='coolwarm_r',alpha=.6)\n",
    "plt.title('Correct label?')\n",
    "\n",
    "# check accuracy:\n",
    "print('Accuracy = ', np.sum(labels==y_true)/np.shape(y_true)[0]*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow point plot\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "distances_all = []\n",
    "for k in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    kmeans_distances = pairwise_distances(X,kmeans.cluster_centers_)\n",
    "    distances_all.append(np.mean(np.min(kmeans_distances,axis=1)))\n",
    "    \n",
    "distances_all = np.array(distances_all)\n",
    "\n",
    "plt.plot(np.arange(10)+1,distances_all)\n",
    "plt.scatter(np.arange(10)+1,distances_all,c='r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Avg distance from cluster centroid')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our overview of scikit-learn. Now let's look at some neuroimaging data with nilearn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
