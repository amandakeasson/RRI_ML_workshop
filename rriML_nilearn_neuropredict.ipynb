{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nilearn\n",
    "\n",
    "There are many helpful tutorials on the nilearn website. Today, we will go through a few of them.\n",
    "\n",
    "Nilearn website: http://nilearn.github.io/auto_examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data using nilearn dataset fetcher\n",
    "haxby_dataset = datasets.fetch_haxby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying representations of different visual stimuli\n",
    "\n",
    "<b>Sources</b>:  \n",
    "\n",
    "http://nilearn.github.io/auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py  \n",
    "\n",
    "http://nilearn.github.io/auto_examples/02_decoding/plot_haxby_different_estimators.html#sphx-glr-auto-examples-02-decoding-plot-haxby-different-estimators-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "df = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "\n",
    "stimuli = df['labels']\n",
    "print(stimuli.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose stimuli of interest\n",
    "task_mask = stimuli.isin(['face', 'house', 'cat'])\n",
    "\n",
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = df['chunks'][task_mask]\n",
    "\n",
    "print(session_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mask of ventral temporal stream \n",
    "\n",
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "plotting.plot_roi(mask_filename, bg_img=haxby_dataset.anat[0],cmap='autumn');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will mask the data using two masks:  \n",
    "1) Mask the observations using the `task_mask` (restrict observations to faces, houses and cats  \n",
    "2) Mask the features (in this case, voxels) using the `voxel_masker` (restrict features to voxels in the ventral temporal mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_masker = NiftiMasker(mask_img=mask_filename, standardize=True)\n",
    "\n",
    "func_filename = haxby_dataset.func[0]\n",
    "\n",
    "masked_timecourses = voxel_masker.fit_transform(func_filename)[task_mask]\n",
    "\n",
    "print('Number of time points:', masked_timecourses.shape[0])\n",
    "print('Number of voxels:', masked_timecourses.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "svm = SVC(C=100., kernel=\"linear\")\n",
    "logistic = LogisticRegression(C=1., penalty=\"l2\", solver='liblinear')\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Make a dictionary to hold all the classifiers\n",
    "classifiers = {'SVM': svm,\n",
    "               'LogReg': logistic,\n",
    "               'Random_Forest': random_forest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "\n",
    "# Make a data splitting object for cross validation\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "import time\n",
    "\n",
    "classifiers_scores = {}\n",
    "categories = stimuli[task_mask].unique()\n",
    "\n",
    "for classifier_name, classifier in sorted(classifiers.items()):\n",
    "    classifiers_scores[classifier_name] = {}\n",
    "    print(70 * '_')\n",
    "\n",
    "    for category in categories:\n",
    "        classification_target = stimuli[task_mask].isin([category])\n",
    "        classifiers_scores[classifier_name][category] = cross_val_score(\n",
    "            classifier,\n",
    "            masked_timecourses,\n",
    "            classification_target,\n",
    "            cv=cv,\n",
    "            groups=session_labels,\n",
    "            scoring=\"accuracy\",\n",
    "        )\n",
    "\n",
    "        print(\"%10s: %14s -- scores: %1.2f +- %1.2f\" %\n",
    "            (classifier_name, category,\n",
    "                classifiers_scores[classifier_name][category].mean(),\n",
    "                classifiers_scores[classifier_name][category].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bar plot\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "tick_position = np.arange(len(categories))\n",
    "plt.xticks(tick_position, categories, rotation=45)\n",
    "\n",
    "for color, classifier_name in zip(['red', 'cyan', 'pink'],sorted(classifiers)):\n",
    "    score_means = [classifiers_scores[classifier_name][category].mean()\n",
    "                   for category in categories]\n",
    "    score_std = [np.std(classifiers_scores[classifier_name][category]) for category in categories]\n",
    "    plt.bar(tick_position, score_means, label=classifier_name,\n",
    "            width=.11, color=color, yerr=score_std)\n",
    "    tick_position = tick_position + .12\n",
    "\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.xlabel('Stimuli category')\n",
    "plt.ylim(ymin=0)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Category-specific classification accuracy for different classifiers')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot face vs. house maps for SVM\n",
    "\n",
    "# Use the average EPI as a background\n",
    "from nilearn import image\n",
    "mean_epi_img = image.mean_img(func_filename)\n",
    "\n",
    "# Restrict the decoding to face vs house\n",
    "condition_mask = stimuli.isin(['face', 'house'])\n",
    "masked_timecourses = masked_timecourses[\n",
    "    condition_mask[task_mask]]\n",
    "stimuli = (stimuli[condition_mask] == 'face')\n",
    "\n",
    "# Transform the stimuli to binary values\n",
    "stimuli.astype(np.int);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(masked_timecourses, stimuli)\n",
    "weights = svm.coef_\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have one weight per voxel. Now, we'll want to convert the weights back into a Nifti image.  \n",
    "\n",
    "We can accomplish this using `inverse_transform` on `voxel_masker`.  \n",
    "\n",
    "Then, we can retrieve the image data by calling `.get_fdata()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_img = voxel_masker.inverse_transform(weights)\n",
    "weight_map = weight_img.get_fdata()\n",
    "\n",
    "print('Shape of nifti data:', weight_map.shape, '\\n')\n",
    "\n",
    "print('Nifti image information:', '\\n')\n",
    "print(weight_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the weights using `plot_stat_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "threshold = np.max(np.abs(weight_map)) * 0.01\n",
    "print(np.max(np.abs(weight_map)))\n",
    "\n",
    "plot_stat_map(weight_img, bg_img=mean_epi_img,\n",
    "              display_mode='z', cut_coords=[-15],\n",
    "              threshold=threshold,\n",
    "              title='%s: face vs house' % classifier_name)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaceNet\n",
    "\n",
    "Source: http://nilearn.github.io/decoding/space_net.html#space-net\n",
    "\n",
    "SpaceNet implements spatial penalties which improve brain decoding power as well as decoder maps.  \n",
    "\n",
    "We will run SpaceNet using the `graph-net` penalty (see [Grosenick et al. 2013](https://www.ncbi.nlm.nih.gov/pubmed/23298747)).  \n",
    "\n",
    "Using graph-net, we can obtain \"sparse but structured solutions by combining structured graph constraints (based on knowledge about coefficient smoothness or connectivity) with a global sparsity-inducing prior that automatically selects important variables.\" (Grosenick et al., 2013)\n",
    "\n",
    "\n",
    "#### Full tutorial:\n",
    "\n",
    "http://nilearn.github.io/auto_examples/02_decoding/plot_haxby_space_net.html#sphx-glr-auto-examples-02-decoding-plot-haxby-space-net-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Restrict to face and house conditions\n",
    "stimuli = df['labels']\n",
    "condition_mask = stimuli.isin(['face', 'house'])\n",
    "\n",
    "# Split data into train and test samples, using the chunks\n",
    "condition_mask_train = (condition_mask) & (df['chunks'] <= 6)\n",
    "condition_mask_test = (condition_mask) & (df['chunks'] > 6)\n",
    "\n",
    "# Apply this sample mask to X (fMRI data) and y (behavioral labels)\n",
    "# Because the data is in one single large 4D image, we need to use index_img to do the split easily\n",
    "from nilearn.image import index_img\n",
    "func_filenames = haxby_dataset.func[0]\n",
    "X_train = index_img(func_filenames, condition_mask_train)\n",
    "X_test = index_img(func_filenames, condition_mask_test)\n",
    "y_train = stimuli[condition_mask_train]\n",
    "y_test = stimuli[condition_mask_test]\n",
    "\n",
    "# Compute the mean epi to be used for the background of the plotting\n",
    "from nilearn.image import mean_img\n",
    "background_img = mean_img(func_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import SpaceNetClassifier\n",
    "\n",
    "# Fit model on train data and predict on test data\n",
    "decoder = SpaceNetClassifier(memory=\"nilearn_cache\", penalty='graph-net')\n",
    "decoder.fit(X_train, y_train)\n",
    "y_pred = decoder.predict(X_test)\n",
    "accuracy = (y_pred == y_test).mean() * 100.\n",
    "print(\"Graph-net classification accuracy : %g%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map, show\n",
    "coef_img = decoder.coef_img_\n",
    "# Save the coefficients to a nifti file\n",
    "coef_img.to_filename('haxby_graph-net_weights.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import load_img\n",
    "accuracy = 77.7778\n",
    "coef_nii = load_img('haxby_graph-net_weights.nii')\n",
    "plot_stat_map(coef_nii, background_img,\n",
    "              title=\"graph-net: accuracy %g%%\" % accuracy,\n",
    "              cut_coords=(-52, -5), display_mode=\"yz\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at some brain networks!\n",
    "\n",
    "<b>Source</b>:  \n",
    "http://nilearn.github.io/auto_examples/03_connectivity/plot_sphere_based_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-sphere-based-connectome-py\n",
    "\n",
    "First, we will extract functional connectivity matrices from the Power atlas for a single participant from the ADHD dataset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd = datasets.fetch_adhd(n_subjects=1)\n",
    "fmri_filename = adhd.func[0]\n",
    "confounds_filename = adhd.confounds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch coordinates of Power atlas\n",
    "power = datasets.fetch_coords_power_2011()\n",
    "coords = np.vstack((power.rois['x'], power.rois['y'], power.rois['z'])).T\n",
    "print(coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a spheres masker\n",
    "\n",
    "from nilearn import input_data\n",
    "\n",
    "spheres_masker = input_data.NiftiSpheresMasker(seeds=coords, smoothing_fwhm=4, radius=5.,\n",
    "    detrend=True, standardize=True, low_pass=0.1, high_pass=0.01, t_r=2.5)\n",
    "\n",
    "# mask the data to obtain the time series\n",
    "timeseries = spheres_masker.fit_transform(fmri_filename, confounds=confounds_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this to Pearson correlation coefficient\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "test_subj = []\n",
    "test_subj.append(timeseries)\n",
    "correlation_matrices = correlation_measure.fit_transform(test_subj)\n",
    "print('Correlation matrices shape: ', correlation_matrices.shape)\n",
    "\n",
    "fc_subj1 = correlation_matrices[0,:,:]\n",
    "\n",
    "plotting.plot_matrix(fc_subj1, vmin=-1., vmax=1., colorbar=True)\n",
    "\n",
    "# keep only the strongest connections\n",
    "# if edge_threshold is a percentage: only the edges with an absolute value above the given percentile will be shown\n",
    "plotting.plot_connectome(fc_subj1, coords, title='Power correlation graph',\n",
    "                         edge_threshold='99.8%', node_size=20, colorbar=True)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group-level analysis: ADHD dataset\n",
    "\n",
    "Source: http://nilearn.github.io/auto_examples/03_connectivity/plot_group_level_connectivity.html#sphx-glr-auto-examples-03-connectivity-plot-group-level-connectivity-py\n",
    "\n",
    "This analysis is performed on 20 participants. It takes a long time to download and extract the data, so I have performed those steps and saved the connectivity matrices, which we will load.  \n",
    "\n",
    "The atlas used here is the MSDL atlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msdl_data = datasets.fetch_atlas_msdl()\n",
    "msdl_coords = msdl_data.region_coords\n",
    "n_regions = len(msdl_coords)\n",
    "networks = []\n",
    "for network in msdl_data.networks:\n",
    "    networks.append(network.decode(\"utf-8\"))\n",
    "print(networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ADHD functional connectivity (FC), which has already been prepared.  \n",
    "\n",
    "We will compare different connectivity metrics: correlation, partial correlation, and tangent. The tangent metric models individual FC as perturbations of the group FC matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ADHD data\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mat = loadmat('adhd_fc_matrices.mat')\n",
    "conn_mat_tmp = mat['connectivity_biomarkers']\n",
    "conn_mat_full_tmp = mat['connectivity_biomarkers_2D']\n",
    "labels = mat['labels'][0].tolist()\n",
    "sites = mat['sites'].tolist()\n",
    "\n",
    "kinds = ['correlation', 'partial correlation', 'tangent']\n",
    "\n",
    "conn_mat = {}\n",
    "conn_mat_full = {}\n",
    "for kind in kinds:\n",
    "    tmp = conn_mat_tmp[kind]\n",
    "    tmp2 = tmp[0][0]\n",
    "    conn_mat[kind] = tmp2\n",
    "    tmp = conn_mat_full_tmp[kind]\n",
    "    tmp2 = tmp[0][0]\n",
    "    conn_mat_full[kind] = tmp2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a model to see which method is best\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "classes = ['{0}{1}'.format(site_name, adhd_label)\n",
    "           for site_name, adhd_label in zip(sites, labels)]\n",
    "cv = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier for each connectivity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mean_scores = []\n",
    "std_scores = []\n",
    "for kind in kinds:\n",
    "    svc = LinearSVC(random_state=0)\n",
    "    cv_scores = cross_val_score(svc, conn_mat[kind],\n",
    "                                y=labels, cv=cv,\n",
    "                                groups=labels, scoring='accuracy')\n",
    "    mean_scores.append(cv_scores.mean())\n",
    "    std_scores.append(np.std(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the CV accuracy for each connectivity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(np.arange(3), mean_scores, align='center', yerr=std_scores,color='red')\n",
    "xticks = [kind.replace(' ', '\\n') for kind in kinds]\n",
    "plt.xticks(np.arange(3), yticks)\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuropredict\n",
    "\n",
    "The final package we will look at today is called Neuropredict. It was developed by Dr. Pradeep Raamana.  \n",
    "\n",
    "You can find the code here: https://github.com/raamana/neuropredict.  \n",
    "\n",
    "Check out https://raamana.github.io/neuropredict for full documentation.  \n",
    "\n",
    "Neuropredict makes it easy to run machine learning pipelines on neuroimaging data (or any data, really!).  \n",
    "\n",
    "We will analyze the above Haxby data using Neuropredict. First, we need to prepare the data so that it is in the correct format for Neuropredict.\n",
    "\n",
    "We will create pandas dataframes, then save the X (time points x voxels) and y (stimuli labels) as .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for neuropredict\n",
    "\n",
    "labels = stimuli[task_mask]\n",
    "\n",
    "obs_names = []\n",
    "f, c, h, = 0, 0, 0\n",
    "for l in labels:\n",
    "    if l == 'face':\n",
    "        f+= 1\n",
    "        obs_names.append('face' + str(f))\n",
    "    elif l == 'cat':\n",
    "        c+= 1\n",
    "        obs_names.append('cat' + str(c))\n",
    "    elif l == 'house':\n",
    "        h+=1\n",
    "        obs_names.append('house' + str(h))\n",
    "        \n",
    "df_features = pd.DataFrame(masked_timecourses)\n",
    "df_labels = pd.DataFrame(np.column_stack([obs_names, labels]))\n",
    "        \n",
    "df_features.to_csv('neuropredict_features.csv', header=False, index=False)\n",
    "df_labels.to_csv('neuropredict_labels.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all you have to do to run neuropredict is run the following command in your terminal/command prompt:  \n",
    "\n",
    "`neuropredict -m neuropredict_labels.csv -d neuropredict_features.csv`\n",
    "\n",
    "Or, you can run it in a Jupyter notebook by adding an exclamation mark at the front of the command: \n",
    "\n",
    "`!neuropredict -m neuropredict_labels.csv -d neuropredict_features.csv`\n",
    "\n",
    "This takes around an hour to run, so I've uploaded the results for us to take a look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
